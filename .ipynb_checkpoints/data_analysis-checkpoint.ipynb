{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# S2ORC Dataset Analysis Notebook\n",
        "\n",
        "This notebook provides tools for analyzing the processed Parquet files from the ETL pipeline.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Make sure you're using the `nvidia_impact_env` conda environment for GPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Try to import cuDF for GPU acceleration (if available)\n",
        "try:\n",
        "    import cudf\n",
        "    USE_GPU = True\n",
        "    print(\"‚úÖ GPU acceleration available (cuDF)\")\n",
        "except ImportError:\n",
        "    USE_GPU = False\n",
        "    print(\"‚ö†Ô∏è  GPU acceleration not available, using Pandas (CPU)\")\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "pd.set_option('display.max_rows', 20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n",
        "\n",
        "Load all Parquet files or specific splits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "PARQUET_DIR = \"processed_parquet\"\n",
        "\n",
        "# Check what files we have\n",
        "train_files = glob.glob(f\"{PARQUET_DIR}/chunk_train_*.parquet\")\n",
        "test_files = glob.glob(f\"{PARQUET_DIR}/chunk_test_*.parquet\")\n",
        "val_files = glob.glob(f\"{PARQUET_DIR}/chunk_val_*.parquet\")\n",
        "\n",
        "print(f\"üìä Parquet Files Found:\")\n",
        "print(f\"   Train: {len(train_files):,} files\")\n",
        "print(f\"   Test:  {len(test_files):,} files\")\n",
        "print(f\"   Val:   {len(val_files):,} files\")\n",
        "print(f\"   Total: {len(train_files) + len(test_files) + len(val_files):,} files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample to inspect structure\n",
        "if USE_GPU:\n",
        "    sample_df = cudf.read_parquet(train_files[0])\n",
        "else:\n",
        "    sample_df = pd.read_parquet(train_files[0])\n",
        "\n",
        "print(f\"üìã Sample file shape: {sample_df.shape}\")\n",
        "print(f\"\\nüìù Columns:\")\n",
        "print(list(sample_df.columns))\n",
        "print(f\"\\nüîç First few rows:\")\n",
        "sample_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all data (or specific split)\n",
        "def load_all_data(use_gpu=USE_GPU, split=None):\n",
        "    \"\"\"Load all parquet files, optionally filtered by split\"\"\"\n",
        "    if split:\n",
        "        pattern = f\"{PARQUET_DIR}/chunk_{split}_*.parquet\"\n",
        "        files = glob.glob(pattern)\n",
        "    else:\n",
        "        files = glob.glob(f\"{PARQUET_DIR}/*.parquet\")\n",
        "    \n",
        "    print(f\"üìÇ Loading {len(files):,} files...\")\n",
        "    \n",
        "    if use_gpu:\n",
        "        df = cudf.read_parquet(files)\n",
        "    else:\n",
        "        # Load in chunks to avoid memory issues\n",
        "        dfs = []\n",
        "        for i, f in enumerate(files):\n",
        "            if i % 100 == 0:\n",
        "                print(f\"   Loading file {i+1}/{len(files)}...\")\n",
        "            dfs.append(pd.read_parquet(f))\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "    \n",
        "    print(f\"‚úÖ Loaded {len(df):,} records\")\n",
        "    return df\n",
        "\n",
        "# Uncomment to load:\n",
        "# df_all = load_all_data()\n",
        "# df_train = load_all_data(split='train')\n",
        "# df_test = load_all_data(split='test')\n",
        "# df_val = load_all_data(split='val')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration\n",
        "\n",
        "Explore the dataset structure and statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "def explore_data(df, name=\"Dataset\"):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìä {name} Statistics\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total records: {len(df):,}\")\n",
        "    print(f\"\\nColumn info:\")\n",
        "    print(df.info())\n",
        "    \n",
        "    if 'year' in df.columns:\n",
        "        print(f\"\\nüìÖ Year range: {df['year'].min()} - {df['year'].max()}\")\n",
        "    \n",
        "    if 'primary_field' in df.columns:\n",
        "        print(f\"\\nüî¨ Top 10 Fields of Study:\")\n",
        "        print(df['primary_field'].value_counts().head(10))\n",
        "    \n",
        "    if 'text_length' in df.columns:\n",
        "        print(f\"\\nüìù Text Length Statistics:\")\n",
        "        print(df['text_length'].describe())\n",
        "\n",
        "# Example usage:\n",
        "# explore_data(sample_df, \"Sample File\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
